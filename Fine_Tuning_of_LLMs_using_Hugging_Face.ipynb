{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNa7+rhrGqa3SEGN52l2Wt2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoSahil147/AI-A-Z-Projects/blob/main/Fine_Tuning_of_LLMs_using_Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning LLMs with Hugging Face"
      ],
      "metadata": {
        "id": "IbodbqhrZil1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Installing and importing the libraries"
      ],
      "metadata": {
        "id": "8GL-x8b1ZpD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ],
      "metadata": {
        "id": "bL3Ft-LeZw02"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgg3OQeSaYIv",
        "outputId": "738a20fa-54a6-4aac-a18d-fef0885d5479"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from datasets import load_dataset\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline)"
      ],
      "metadata": {
        "id": "KSxtvOuTafFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f691e81d-2dc4-4d70-fb2a-b1c1a86d7cbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Loading the model"
      ],
      "metadata": {
        "id": "y0rOdPyHaoP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "if device == \"cuda\":\n",
        "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"aboonaji/llama2finetune-v2\",\n",
        "        device_map=\"auto\",\n",
        "        max_memory={0: \"8GB\"},\n",
        "        offload_folder=\"./offload\"\n",
        "    )\n",
        "else:\n",
        "    llama_model = AutoModelForCausalLM.from_pretrained(\"aboonaji/llama2finetune-v2\")\n",
        "\n",
        "llama_model.config.use_cache = False\n",
        "llama_model.config.pretraining_tp = 1\n",
        "\n",
        "if device == \"cuda\":\n",
        "    # Enable gradient checkpointing only on GPU\n",
        "    llama_model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGLNzcn4atCF",
        "outputId": "ac26f5f6-affa-40da-c586-58c63491e40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Loading the tokenizer"
      ],
      "metadata": {
        "id": "88ZCcpG8b85a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(\"aboonaji/llama2finetune-v2\", trust_remote_code=True)\n",
        "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "Bym2gKmvb-2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Setting the training arguments"
      ],
      "metadata": {
        "id": "h1GipWjpcZu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "if device == \"cuda\":\n",
        "    training_arguments = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=1,\n",
        "        max_steps=100,\n",
        "        fp16=True  # Use fp16 for GPU (CUDA)\n",
        "    )\n",
        "else:\n",
        "    training_arguments = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=1,\n",
        "        max_steps=100,\n",
        "        bf16=True  # Use bf16 if on TPU; or remove if unsupported\n",
        "    )"
      ],
      "metadata": {
        "id": "7v_M7Q5WccG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Creating the Supervised Fine-Tuning trainer"
      ],
      "metadata": {
        "id": "efaPVWrYcoE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Initialize the trainer with a further reduced max sequence length (e.g., 256 tokens)\n",
        "llama_sft_trainer = SFTTrainer(\n",
        "    model=llama_model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=load_dataset(path=\"aboonaji/wiki_medical_terms_llam2_format\", split=\"train\"),\n",
        "    tokenizer=llama_tokenizer,\n",
        "    peft_config=LoraConfig(task_type=\"CAUSAL_LM\", r=64, lora_alpha=16, lora_dropout=0.1),\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=256  # Further reduce the sequence length to lower memory usage\n",
        ")"
      ],
      "metadata": {
        "id": "kt_atvyDcpkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Training the model"
      ],
      "metadata": {
        "id": "blGxXeyjcvf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llama_sft_trainer.train()"
      ],
      "metadata": {
        "id": "bo3CerqAcyTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Chatting with the model"
      ],
      "metadata": {
        "id": "vjIKdogsc4yB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Please tell me about Ascariasis\"\n",
        "text_generation_pipeline = pipeline(task = \"text-generation\", model = llama_model, tokenizer = llama_tokenizer, max_length = 300)\n",
        "model_answer = text_generation_pipeline(f\"<s>[INST] {user_prompt} [/INST]\")\n",
        "print(model_answer[0]['generated_text'])"
      ],
      "metadata": {
        "id": "DMxQ_fGjc6lr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}